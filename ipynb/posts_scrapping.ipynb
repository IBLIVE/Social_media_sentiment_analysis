{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc0roMHcIscv",
    "outputId": "e8b511d1-6345-419d-f8ce-1fac2c53f52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Mastodon.py in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: requests>=2.4.2 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (1.16.0)\n",
      "Requirement already satisfied: decorator>=4.0.0 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (5.1.1)\n",
      "Requirement already satisfied: blurhash>=1.1.4 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (1.1.4)\n",
      "Requirement already satisfied: python-magic-bin in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from Mastodon.py) (0.4.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests>=2.4.2->Mastodon.py) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests>=2.4.2->Mastodon.py) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests>=2.4.2->Mastodon.py) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from requests>=2.4.2->Mastodon.py) (2024.2.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\pc\\.conda\\envs\\deeplearning\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install Mastodon.py\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_Xrro1OIugm",
    "outputId": "7fac7a88-2615-4964-f40a-b58b96874956"
   },
   "outputs": [],
   "source": [
    "from mastodon import Mastodon\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "#folder_path = '/colab_json_files/' \n",
    "#if not os.path.exists(folder_path):\n",
    "#    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ZiZo07scZ1UU"
   },
   "outputs": [],
   "source": [
    "mastodon = Mastodon(access_token='YvMBnWgknUL41MI3QjRaOn7goLZ1rRpgyHWZjrxIF9E', api_base_url='https://mastodon.social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rate_limiting(mastodon):\n",
    "    remaining_requests = mastodon.ratelimit_remaining\n",
    "    reset_time = mastodon.ratelimit_reset\n",
    "    current_time = time.time()\n",
    "\n",
    "    if remaining_requests == 0 and reset_time:\n",
    "        sleep_duration = reset_time - current_time + 1\n",
    "        if sleep_duration > 0:\n",
    "            print(f\"Rate limit reached. Sleeping for {sleep_duration:.2f} seconds.\")\n",
    "            time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mF5anMTBaRUI"
   },
   "outputs": [],
   "source": [
    "hashtags = ['trump', 'donaldtrump', 'kamalaharris', 'harris'] #hashtags I used for search\n",
    "#hashtags = ['donaldtrump'] \n",
    "data_json = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "cTjbZHAmkyuu"
   },
   "outputs": [],
   "source": [
    "def get_text(content):\n",
    "  soup = BeautifulSoup(content, 'html.parser')\n",
    "  return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "uGpd4mYSzDFc"
   },
   "outputs": [],
   "source": [
    "def get_replies_and_ids(id, data, hashtag):\n",
    "    dt = []\n",
    "    context = mastodon.status_context(id)\n",
    "    handle_rate_limiting(mastodon)\n",
    "    replies = context['descendants']\n",
    "    \n",
    "    for reply in replies:\n",
    "        reply_data = {\n",
    "        'id': reply['id'],\n",
    "        'content': get_text(reply['content']),\n",
    "        'replies': [],\n",
    "        'toxicity': 0,\n",
    "        'hashtag': hashtag\n",
    "        }\n",
    "        descendant_replies = get_replies_and_ids(reply['id'], data, hashtag)\n",
    "        if descendant_replies:\n",
    "          reply_data['replies'].extend(descendant_replies)\n",
    "        data[reply['id']] = reply_data\n",
    "        dt.append(reply['id'])\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Iz81f8Jvk5vL"
   },
   "outputs": [],
   "source": [
    "#def get_posts_and_replies(hashtag, limit=130, page_limit=20):\n",
    "#    print(\"Starting collecting data for hashtag #\" + hashtag)\n",
    "#    data = dict()\n",
    "#    #posts = mastodon.timeline_hashtag(hashtag, limit=limit)\n",
    "#    max_id = None\n",
    "#    total_collected = 0\n",
    "#\n",
    "#    while total_collected < limit:\n",
    "#        print(f\"Starting collecting from {total_collected} posts for hashtag #{hashtag}\")\n",
    "#        batch_limit = min(page_limit, limit-total_collected)\n",
    "#        posts = mastodon.timeline_hashtag(hashtag, limit=batch_limit, max_id=max_id)\n",
    "#\n",
    "#        if not posts:\n",
    "#            print(f\"No more posts to fetch for hashtag #{hashtag}\")\n",
    "#            break\n",
    "#\n",
    "#        for post in posts:\n",
    "#            post_data = {\n",
    "#            'id': post['id'],\n",
    "#            'content': get_text(post['content']),\n",
    "#            'replies': [],\n",
    "#            'toxicity': 0,\n",
    "#            'hashtag': hashtag\n",
    "#            }\n",
    "#            post_data['replies'].extend(get_replies_and_ids(post['id'], data, hashtag))\n",
    "#            data[post['id']] = post_data\n",
    "#\n",
    "#        max_id = posts[-1]['id']\n",
    "#        total_collected += len(posts)\n",
    "#        print(f\"Collected {total_collected} posts for hashtag #{hashtag}\")\n",
    "#    print(\"Collected data from hashtag #\" + hashtag)\n",
    "#    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_and_replies(hashtag, limit=130, page_limit=40):\n",
    "    print(\"Starting collecting data for hashtag #\" + hashtag)\n",
    "    data = dict()\n",
    "    max_id = None\n",
    "    total_collected = 0\n",
    "\n",
    "    while total_collected < limit:\n",
    "        print(f\"Starting collecting from {total_collected} posts for hashtag #{hashtag}\")\n",
    "        batch_limit = min(page_limit, limit - total_collected)\n",
    "        posts = mastodon.timeline_hashtag(hashtag, limit=batch_limit, max_id=max_id)\n",
    "        handle_rate_limiting(mastodon)\n",
    "\n",
    "        if not posts:\n",
    "            print(f\"No more posts to fetch for hashtag #{hashtag}\")\n",
    "            break\n",
    "\n",
    "        for post in posts:\n",
    "            post_data = {\n",
    "                'id': post['id'],\n",
    "                'content': get_text(post['content']),\n",
    "                'replies': [],\n",
    "                'toxicity': 0,\n",
    "                'hashtag': hashtag\n",
    "            }\n",
    "            post_data['replies'].extend(get_replies_and_ids(post['id'], data, hashtag))\n",
    "            data[post['id']] = post_data\n",
    "\n",
    "        max_id = posts[-1]['id']\n",
    "        total_collected += len(posts)\n",
    "        print(f\"Collected {total_collected} posts for hashtag #{hashtag}\")\n",
    "\n",
    "    print(\"Collected data from hashtag #\" + hashtag)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RggtUYvSaanU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collecting data for hashtag #donaldtrump\n",
      "Starting collecting from 0 posts for hashtag #donaldtrump\n",
      "Collected 40 posts for hashtag #donaldtrump\n",
      "Collected data from hashtag #donaldtrump\n"
     ]
    }
   ],
   "source": [
    "for hashtag in hashtags:\n",
    "  data_json = get_posts_and_replies(hashtag)\n",
    "  with open(f'{hashtag}.json', 'w') as f:\n",
    "    json.dump(data_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: trump.json\n",
      "Number of entries: 269\n",
      "Sample entry - ID: 113301683458757153\n",
      "Sample data keys: id, content, replies, toxicity, hashtag\n",
      "File: harris.json\n",
      "Number of entries: 307\n",
      "Sample entry - ID: 113299532808803896\n",
      "Sample data keys: id, content, replies, toxicity, hashtag\n",
      "File: donaldtrump.json\n",
      "Number of entries: 206\n",
      "Sample entry - ID: 113301790869024476\n",
      "Sample data keys: id, content, replies, toxicity, hashtag\n",
      "File: kamalaharris.json\n",
      "Number of entries: 206\n",
      "Sample entry - ID: 113299439517108438\n",
      "Sample data keys: id, content, replies, toxicity, hashtag\n"
     ]
    }
   ],
   "source": [
    "def analyze_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        entry_count = len(data)\n",
    "        print(f\"File: {os.path.basename(file_path)}\")\n",
    "        print(f\"Number of entries: {entry_count}\")\n",
    "\n",
    "        if entry_count > 0:\n",
    "            sample_id = next(iter(data))\n",
    "            sample_data = data[sample_id]\n",
    "            print(f\"Sample entry - ID: {sample_id}\")\n",
    "            if isinstance(sample_data, dict):\n",
    "                print(f\"Sample data keys: {', '.join(sample_data.keys())}\")\n",
    "    finally:\n",
    "        return\n",
    "\n",
    "def count_contents(target_files):\n",
    "    for file_name in target_files:\n",
    "        if os.path.exists(file_name):\n",
    "            analyze_json_file(file_name)\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "jsons = ['trump.json', 'harris.json', 'donaldtrump.json', 'kamalaharris.json']\n",
    "count_contents(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json(hashtags):\n",
    "    combined_data = {}\n",
    "\n",
    "    for hashtag in hashtags:\n",
    "        file = f'{hashtag}.json'\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for post_id, post_data in data.items():\n",
    "                if post_id in combined_data:\n",
    "                    combined_data[post_id]['hashtag'].append(hashtag)\n",
    "                else:\n",
    "                    combined_data[post_id] = post_data\n",
    "                    combined_data[post_id]['hashtag'] = [hashtag]\n",
    "        else:\n",
    "            print(f'File {file}.json not found')\n",
    "            \n",
    "    return combined_data\n",
    "\n",
    "def save_json(file_name, data):\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['trump', 'harris', 'donaldtrump', 'kamalaharris']\n",
    "combined_data = combine_json(hashtags)\n",
    "file_name = 'data.json'\n",
    "save_json(file_name, combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n",
      "File: data.json\n",
      "Number of entries: 932\n",
      "Sample entry - ID: 113301683458757153\n",
      "Sample data keys: id, content, replies, toxicity, hashtag\n"
     ]
    }
   ],
   "source": [
    "print('Total number of posts and replies:', len(combined_data))\n",
    "analyze_json_file(file_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
